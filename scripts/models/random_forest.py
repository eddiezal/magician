from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from base_model import BaseModel


class RandomForestModel(BaseModel):
    """
    ğŸŒ² Random Forest Model extending BaseModel.  
    Think of it as your **low-maintenance, high-output** employee.  
    No burnout, no excuses, just trees doing their job.  

    âœ… Supports:
        - Training (because raw data wonâ€™t trade itself)  
        - Predictions (hopefully better than your last impulse buy)  
        - Hyperparameter tuning (because â€˜default settingsâ€™ are for amateurs)  
        - Model persistence (so you donâ€™t have to retrain every time you sneeze)  
    """

    def __init__(self, n_estimators=100, max_depth=None):
        """
        Initializes the Random Forest model with settings that wonâ€™t make you regret your life choices.

        :param n_estimators: How many trees in the forest. More = better (usually).  
                            But donâ€™t get greedyâ€”too many trees, and your model moves slower than a boomer opening a new tab.  
        :param max_depth: How deep the trees go.  
                          Set it too high, and your model overfits faster than a trader maxing out leverage on a meme coin.  
        """
        super().__init__("random_forest")
        self.model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)

    def train(self, X_train, y_train):
        """
        Train the Random Forest model on your carefully chosen, definitely-not-biased data.

        :param X_train: Features for training  
        :param y_train: Target values  
        """
        if X_train is None or y_train is None:
            raise ValueError("ğŸš¨ Missing data! Even the best strategies need actual numbers to work with.")

        print("ğŸš€ Training Random Forest Model... This might take longer than your morning coffee, but itâ€™s worth it.")
        self.model.fit(X_train, y_train)
        self.save_model()
        print("âœ… Model trained and saved! Letâ€™s hope it doesnâ€™t predict like a Magic 8-ball.")

    def predict(self, X):
        """
        Generate predictions. Because thatâ€™s literally the whole point.

        :param X: Input features  
        :return: Predictions (that may or may not be better than your gut feeling)  
        """
        if self.model:
            return self.model.predict(X)
        else:
            raise RuntimeError("âš ï¸ Model not trained. Even â€˜diamond handsâ€™ need a backtest.")

    def tune_hyperparameters(self, X_train, y_train):
        """
        Tune hyperparameters using GridSearchCV. Because one-size-fits-all doesnâ€™t apply to trading.

        :param X_train: Features for training  
        :param y_train: Target values  
        """
        if X_train is None or y_train is None:
            raise ValueError("ğŸš¨ You canâ€™t optimize what doesnâ€™t exist. Feed the model some data.")

        print("ğŸ” Tuning Hyperparameters... because â€˜set it and forget itâ€™ is not a strategy.")

        param_grid = {
            "n_estimators": [50, 100, 200],
            "max_depth": [None, 10, 20],
            "min_samples_split": [2, 5, 10],
        }

        grid_search = GridSearchCV(
            RandomForestRegressor(random_state=42), 
            param_grid, 
            cv=3, 
            scoring="r2", 
            verbose=1, 
            n_jobs=-1
        )

        grid_search.fit(X_train, y_train)
        self.model = grid_search.best_estimator_
        self.save_model()

        print(f"âœ… Best Parameters: {grid_search.best_params_} (Finally, something optimized better than your morning routine.)")
        print("âœ… Model is locked and loaded! Now go make some trades.")
